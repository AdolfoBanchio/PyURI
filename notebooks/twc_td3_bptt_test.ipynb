{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cbda22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "SRC_ROOT = \"../src\"\n",
    "if str(SRC_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96b6e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import twc.twc_io\n",
    "import json\n",
    "import os\n",
    "from typing import Callable\n",
    "from torch import nn\n",
    "from fiuri import FIURIModule, FiuriDenseConn, FiuriSparseGJConn\n",
    "from twc.w_builder import build_tw_matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "629b90fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TWC_BPTT(nn.Module):\n",
    "    \"\"\"\n",
    "    Refactored TWC Module for explicit BPTT.\n",
    "    \n",
    "    - forward(): Used for BPTT. Accepts an observation 'x' and an explicit 'state_in',\n",
    "                 returns (action, state_out). This is a \"pure\" function.\n",
    "                 \n",
    "    - get_action_from_obs(): Used for evaluation/sampling. Manages 'self._state'\n",
    "                             internally. This is stateful.\n",
    "                             \n",
    "    - reset(): Resets the internal 'self._state' for evaluation.\n",
    "    - get_initial_state(): Gets a batched, zeroed state for starting BPTT.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_layer, hid_layer, out_layer, in2hid_IN, in2hid_GJ,\n",
    "                 hid_IN, hid_EX, hid2out_EX, obs_encoder, action_decoder,\n",
    "                 internal_steps=1, log_stats=False):\n",
    "        super().__init__()\n",
    "        # neuron layers\n",
    "        self.in_layer = in_layer\n",
    "        self.hid_layer = hid_layer\n",
    "        self.out_layer = out_layer\n",
    "\n",
    "        # connections\n",
    "        self.in2hid_IN = in2hid_IN\n",
    "        self.in2hid_GJ = in2hid_GJ\n",
    "        self.hid_IN = hid_IN\n",
    "        self.hid_EX = hid_EX\n",
    "        self.hid2out = hid2out_EX\n",
    "\n",
    "        # I/O\n",
    "        self.obs_encoder = obs_encoder\n",
    "        self.action_decoder = action_decoder\n",
    "\n",
    "        # MONITOR\n",
    "        self.log = log_stats\n",
    "        self.monitor = {\"in\": [], \"hid\": [], \"out\": []}\n",
    "        self.internal_steps = internal_steps\n",
    "        \n",
    "        # Internal state for get_action_from_obs (evaluation)\n",
    "        self._state = None\n",
    "\n",
    "    def _init_layer_state(self, layer: FIURIModule, batch_size: int, device, dtype):\n",
    "        E0 = torch.full((batch_size, layer.num_cells), layer._init_E, device=device, dtype=dtype)\n",
    "        O0 = torch.full((batch_size, layer.num_cells), layer._init_O, device=device, dtype=dtype)\n",
    "        return (E0, O0)\n",
    "\n",
    "    def get_initial_state(self, batch_size: int, device, dtype=torch.float32) -> dict:\n",
    "        \"\"\"Returns a batched, zeroed state dict for starting a BPTT unroll.\"\"\"\n",
    "        return {\n",
    "            \"in\": self._init_layer_state(self.in_layer, batch_size, device, dtype),\n",
    "            \"hid\": self._init_layer_state(self.hid_layer, batch_size, device, dtype),\n",
    "            \"out\": self._init_layer_state(self.out_layer, batch_size, device, dtype),\n",
    "        }\n",
    "\n",
    "    def forward(self, x: torch.Tensor, state_in: dict) -> torch.Tensor | dict:\n",
    "        \"\"\"\n",
    "        Performs one recurrent step *for training*. (Pure function)\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): The observation, e.g., (B, obs_dim).\n",
    "            state_in (dict): The explicit state from the previous time step.\n",
    "            \n",
    "        Returns:\n",
    "            (torch.Tensor, dict): A tuple of (action, state_out).\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        if x.device != device:\n",
    "            x = x.to(device)\n",
    "            \n",
    "        ex_in, in_in = self.obs_encoder(x, n_inputs=4, device=device)\n",
    "        \n",
    "        # Create a NEW dictionary for the output state.\n",
    "        state_out = {}\n",
    "\n",
    "        # --- Input Layer ---\n",
    "        in_state_in = state_in[\"in\"] # Read from input state\n",
    "        in_out, new_in_state = self.in_layer(ex_in + in_in, state=in_state_in)\n",
    "        state_out[\"in\"] = new_in_state # Write to output state\n",
    "\n",
    "        # --- Hidden Layer (with internal steps) ---\n",
    "        hid_state_in = state_in[\"hid\"] # Read from input state\n",
    "        hid_out = None\n",
    "        \n",
    "        # Note: We use hid_state_tensor to hold the state *within* the internal loop\n",
    "        hid_state_tensor = hid_state_in \n",
    "        \n",
    "        for _ in range(self.internal_steps):\n",
    "            in2hid_influence = self.in2hid_IN(in_out)\n",
    "            in2hid_gj_bundle = self.in2hid_GJ(in_out)\n",
    "            \n",
    "            hid_out, hid_state_tensor = self.hid_layer(\n",
    "                in2hid_influence,\n",
    "                state=hid_state_tensor, # Use most recent state\n",
    "                gj_bundle=in2hid_gj_bundle,\n",
    "                o_pre=in_out,\n",
    "            )\n",
    "            hid_ex_influence = self.hid_EX(hid_out)\n",
    "            hid_in_influence = self.hid_IN(hid_out)\n",
    "            hid_out, hid_state_tensor = self.hid_layer(\n",
    "                hid_ex_influence + hid_in_influence,\n",
    "                state=hid_state_tensor, # Use most recent state\n",
    "            )\n",
    "\n",
    "        state_out[\"hid\"] = hid_state_tensor # Write final internal state\n",
    "\n",
    "        # --- Output Layer ---\n",
    "        hid2out_ex_influence = self.hid2out(hid_out)\n",
    "        out_state_tensor, out_layer_state = self.out_layer(hid2out_ex_influence, state=state_in[\"out\"])\n",
    "        state_out[\"out\"] = out_layer_state\n",
    "        \n",
    "        if self.log:\n",
    "            self.log_monitor(state_out)\n",
    "            \n",
    "        return self.action_decoder(out_state_tensor), state_out\n",
    "\n",
    "    # --- This is your original 'forward' method, renamed for evaluation ---\n",
    "    def get_action_from_obs(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        One TWC step for *evaluation/sampling*. (Stateful)\n",
    "        This method uses and updates self._state internally.\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        if x.device != device:\n",
    "            x = x.to(device)\n",
    "            \n",
    "        ex_in, in_in = self.obs_encoder(x, n_inputs=4, device=device)\n",
    "        B = ex_in.size(0)\n",
    "        dtype = ex_in.dtype\n",
    "        \n",
    "        # Use and manage internal state\n",
    "        state = self._ensure_state(B, device, dtype)\n",
    "        \n",
    "        # Detach the state for inference\n",
    "        state = {name: (pair[0].detach(), pair[1].detach()) for name, pair in state.items()}\n",
    "\n",
    "        in_state = state[\"in\"]\n",
    "        in_out, new_in_state = self.in_layer(ex_in + in_in, state=in_state)\n",
    "        state[\"in\"] = new_in_state # Inplace update of local 'state' dict\n",
    "\n",
    "        hid_state = state[\"hid\"]\n",
    "        hid_out = None\n",
    "        \n",
    "        for _ in range(self.internal_steps):\n",
    "            in2hid_influence = self.in2hid_IN(in_out)\n",
    "            in2hid_gj_bundle = self.in2hid_GJ(in_out)\n",
    "            \n",
    "            hid_out, hid_state = self.hid_layer(\n",
    "                in2hid_influence,\n",
    "                state=hid_state,\n",
    "                gj_bundle=in2hid_gj_bundle,\n",
    "                o_pre=in_out,\n",
    "            )\n",
    "            hid_ex_influence = self.hid_EX(hid_out)\n",
    "            hid_in_influence = self.hid_IN(hid_out)\n",
    "            hid_out, hid_state = self.hid_layer(\n",
    "                hid_ex_influence + hid_in_influence,\n",
    "                state=hid_state,\n",
    "            )\n",
    "            state[\"hid\"] = hid_state # Inplace update\n",
    "\n",
    "        \n",
    "        hid2out_ex_influence = self.hid2out(hid_out)\n",
    "        out_state_tensor, out_layer_state = self.out_layer(hid2out_ex_influence, state=state[\"out\"])\n",
    "        state[\"out\"] = out_layer_state\n",
    "        \n",
    "        self._state = state # Overwrite internal state\n",
    "        \n",
    "        if self.log:\n",
    "            self.log_monitor(state)\n",
    "            \n",
    "        return self.action_decoder(out_state_tensor)\n",
    "\n",
    "    # --- Helper methods for evaluation state ---\n",
    "    def _make_state(self, batch_size: int, device, dtype):\n",
    "        return self.get_initial_state(batch_size, device, dtype)\n",
    "\n",
    "    def _ensure_state(self, batch_size: int, device, dtype):\n",
    "        if self._state is None:\n",
    "            self._state = self._make_state(batch_size, device, dtype)\n",
    "            return self._state\n",
    "\n",
    "        sample_E, _ = self._state[\"in\"]\n",
    "        if sample_E.shape[0] != batch_size or sample_E.device != device or sample_E.dtype != dtype:\n",
    "            self._state = self._make_state(batch_size, device, dtype)\n",
    "        return self._state\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the internal state variables for evaluation.\"\"\"\n",
    "        self._state = None\n",
    "    \n",
    "    # --- Other helpers ---\n",
    "    def log_monitor(self, state):\n",
    "        def _pack(layer, state_pair):\n",
    "            return {\n",
    "                \"in_state\": state_pair[0].detach().cpu(),\n",
    "                \"out_state\": state_pair[1].detach().cpu(),\n",
    "                \"threshold\": layer.threshold.detach().cpu(),\n",
    "                \"decay_factor\": layer.decay.detach().cpu(),\n",
    "            }\n",
    "        self.monitor[\"in\"].append(_pack(self.in_layer, state[\"in\"]))\n",
    "        self.monitor[\"hid\"].append(_pack(self.hid_layer, state[\"hid\"]))\n",
    "        self.monitor[\"out\"].append(_pack(self.out_layer, state[\"out\"]))\n",
    "\n",
    "\n",
    "# --- This is the builder function, now separate from the class ---\n",
    "\n",
    "def create_layer(n_neurons) -> FIURIModule:\n",
    "    return FIURIModule(\n",
    "        num_cells=n_neurons,\n",
    "        initial_in_state=0.0,\n",
    "        initial_out_state=0.0,\n",
    "        initial_threshold=0.0,\n",
    "        initial_decay=0.1,\n",
    "        clamp_min=-10.0,\n",
    "        clamp_max=10.0,\n",
    "    )\n",
    "\n",
    "json_path = \"TWC_fiu.json\"\n",
    "\n",
    "def build_twc(obs_encoder: Callable,\n",
    "              action_decoder: Callable,\n",
    "              internal_steps: int,\n",
    "              log_stats: bool = True) -> nn.Module:\n",
    "    \"\"\"Builds and returns a TWC_BPTT model.\"\"\"\n",
    "    \n",
    "    with open(json_path, \"r\") as f:\n",
    "        net_data = json.load(f)\n",
    "\n",
    "    masks, sizes = build_tw_matrices(net_data)\n",
    "\n",
    "    n_in, n_hid, n_out = sizes[\"n_in\"], sizes[\"n_hid\"], sizes[\"n_out\"]\n",
    "\n",
    "    in_layer =  create_layer(n_in)\n",
    "    hid_layer = create_layer(n_hid)\n",
    "    out_layer = create_layer(n_out)\n",
    "\n",
    "    in2hid = FiuriDenseConn(n_pre=n_in, n_post=n_hid,w_mask=masks[\"in2hid\"][\"IN\"], type=\"IN\")\n",
    "    hid_IN = FiuriDenseConn(n_pre=n_hid, n_post=n_hid, w_mask=masks[\"hid\"][\"IN\"], type=\"IN\")\n",
    "    hid_EX = FiuriDenseConn(n_pre=n_hid, n_post=n_hid, w_mask=masks[\"hid\"][\"EX\"], type=\"EX\")\n",
    "    hid2out_EX = FiuriDenseConn(n_pre=n_hid, n_post=n_out, w_mask=masks[\"hid2out\"][\"EX\"], type=\"EX\")\n",
    "\n",
    "    # create the only GJ sparse conn\n",
    "    # PLM -> PVC, AVM -> AVD\n",
    "    gj_edges = torch.tensor([[1, 2],   # src (PLM=1, AVM=2)\n",
    "                             [2, 1]])  # dst (PVC=2, AVD=1)\n",
    "    gj_conn = FiuriSparseGJConn(n_pre=n_in, n_post=n_hid, gj_edges=gj_edges)\n",
    "\n",
    "    model = TWC_BPTT(\n",
    "        in_layer=in_layer,\n",
    "        hid_layer=hid_layer,\n",
    "        out_layer=out_layer,\n",
    "        in2hid_IN=in2hid,\n",
    "        in2hid_GJ=gj_conn,\n",
    "        hid_IN=hid_IN,\n",
    "        hid_EX=hid_EX,\n",
    "        hid2out_EX=hid2out_EX,\n",
    "        obs_encoder=obs_encoder,\n",
    "        action_decoder=action_decoder,\n",
    "        internal_steps=internal_steps,\n",
    "        log_stats=log_stats\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83c4ca30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('in_layer.threshold', tensor([0., 0., 0., 0.])), ('in_layer.decay', tensor([0.1000, 0.1000, 0.1000, 0.1000])), ('hid_layer.threshold', tensor([0., 0., 0., 0., 0.])), ('hid_layer.decay', tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000])), ('out_layer.threshold', tensor([0., 0.])), ('out_layer.decay', tensor([0.1000, 0.1000])), ('in2hid_IN.w', tensor([[-0.6523, -0.2679, -0.5385,  1.0235, -0.5084],\n",
      "        [ 0.6150, -0.6969,  0.4808, -0.6213,  0.2592],\n",
      "        [-0.7714, -0.7895,  0.6727,  0.4728,  0.1473],\n",
      "        [-0.3614, -0.1296,  0.4842,  0.4939,  1.0783]])), ('in2hid_IN.w_mask', tensor([[1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 1., 0., 0.]])), ('in2hid_GJ.gj_w', tensor([-0.1756, -0.1035])), ('in2hid_GJ.gj_idx', tensor([[1, 2],\n",
      "        [2, 1]])), ('hid_IN.w', tensor([[ 0.3388, -0.3819,  0.9485,  0.1069,  0.7444],\n",
      "        [-0.2605,  0.9165, -0.1696, -0.9861,  0.8585],\n",
      "        [-0.0415, -0.4697,  0.7382, -0.8484,  0.4846],\n",
      "        [ 0.9054, -0.0950, -1.0205, -0.3839,  1.0464],\n",
      "        [ 0.5538, -0.2815, -0.1263, -0.8245, -0.8170]])), ('hid_IN.w_mask', tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0.]])), ('hid_EX.w', tensor([[-0.9638,  0.7393,  0.4601,  0.6219, -0.7784],\n",
      "        [ 0.4877,  0.9281, -0.8211,  0.5695, -0.3876],\n",
      "        [ 0.9150, -0.7654, -0.2786,  0.6517,  0.3810],\n",
      "        [-0.8975, -0.4189,  0.5348,  0.1370, -0.9108],\n",
      "        [ 0.9366,  0.8873, -0.0436, -0.4421,  0.9425]])), ('hid_EX.w_mask', tensor([[0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 1., 1.],\n",
      "        [1., 1., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])), ('hid2out.w', tensor([[-1.6313, -0.3241],\n",
      "        [ 0.5803, -0.0558],\n",
      "        [-1.4733,  0.4266],\n",
      "        [ 1.5808, -0.6504],\n",
      "        [-0.0056, -1.1507]])), ('hid2out.w_mask', tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]]))])\n"
     ]
    }
   ],
   "source": [
    "net = build_twc(twc.twc_io.mcc_obs_encoder_speed_weighted, twc.twc_io.twc_out_2_mcc_action_tanh, internal_steps=1)\n",
    "print(net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbdfc462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceBuffer:\n",
    "    \"\"\"\n",
    "    A replay buffer that stores and samples sequences of transitions for BPTT.\n",
    "    \n",
    "    This buffer stores entire episodes. When sampling, it pulls a fixed-length\n",
    "    sequence from a random episode, ensuring that the sequence never\n",
    "    crosses an episode boundary.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            capacity: The maximum number of *transitions* (not episodes) to store.\n",
    "        \"\"\"\n",
    "        # Buffer of episodes. Each episode is a dict of numpy arrays.\n",
    "        self.episodes = deque()\n",
    "        \n",
    "        self.capacity = int(capacity)\n",
    "        self.total_transitions = 0\n",
    "        \n",
    "        # Temporary buffer for the episode currently being collected\n",
    "        self._init_current_episode()\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        \"\"\"Returns the total number of transitions stored in the buffer.\"\"\"\n",
    "        return self.total_transitions\n",
    "\n",
    "    def _init_current_episode(self):\n",
    "        \"\"\"Resets the temporary episode buffer.\"\"\"\n",
    "        self.current_episode = {\n",
    "            \"obs\": [],\n",
    "            \"action\": [],\n",
    "            \"reward\": [],\n",
    "            \"next_obs\": [],\n",
    "            \"done\": [],\n",
    "        }\n",
    "\n",
    "    def store(self, obs: np.ndarray, action: np.ndarray, reward: float, next_obs: np.ndarray, done: bool, truncated: bool):\n",
    "        \"\"\"\n",
    "        Stores a single transition. If 'done' or 'truncated' is True,\n",
    "        the current episode is \"flushed\" to the main buffer.\n",
    "        \"\"\"\n",
    "        # Ensure action is always at least 1D\n",
    "        if not isinstance(action, np.ndarray):\n",
    "             action = np.array([action])\n",
    "        elif action.shape == (): # Handle 0-dim arrays\n",
    "            action = np.array([action.item()])\n",
    "\n",
    "        self.current_episode[\"obs\"].append(obs)\n",
    "        self.current_episode[\"action\"].append(action)\n",
    "        self.current_episode[\"reward\"].append(np.array([reward])) # Store as (1,)\n",
    "        self.current_episode[\"next_obs\"].append(next_obs)\n",
    "        self.current_episode[\"done\"].append(np.array([done])) # Store as (1,)\n",
    "        \n",
    "        if done or truncated:\n",
    "            self._flush_current_episode()\n",
    "\n",
    "    def _flush_current_episode(self):\n",
    "        ep_len = len(self.current_episode[\"obs\"])\n",
    "        if ep_len == 0:\n",
    "            return\n",
    "\n",
    "        flushed_episode = {}\n",
    "        for key in self.current_episode.keys():\n",
    "            flushed_episode[key] = np.stack(self.current_episode[key])\n",
    "            \n",
    "        self.episodes.append(flushed_episode)\n",
    "        self.total_transitions += ep_len\n",
    "        \n",
    "        while self.total_transitions > self.capacity:\n",
    "            evicted_episode = self.episodes.popleft()\n",
    "            self.total_transitions -= len(evicted_episode[\"obs\"])\n",
    "            \n",
    "        self._init_current_episode()\n",
    "\n",
    "    def sample(self, batch_size: int, sequence_length: int, device: torch.device) -> dict:\n",
    "        \"\"\"\n",
    "        Samples a batch of transition sequences for BPTT.\n",
    "        \"\"\"\n",
    "        \n",
    "        valid_episodes = [ep for ep in self.episodes if len(ep[\"obs\"]) >= sequence_length]\n",
    "        \n",
    "        if not valid_episodes:\n",
    "            raise ValueError(f\"Not enough data to sample sequences. Need episodes >= {sequence_length} steps.\")\n",
    "\n",
    "        batch_seq = {key: [] for key in self.current_episode.keys()}\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            ep = random.choice(valid_episodes)\n",
    "            max_start_idx = len(ep[\"obs\"]) - sequence_length\n",
    "            start = np.random.randint(0, max_start_idx + 1)\n",
    "            end = start + sequence_length\n",
    "            \n",
    "            for key in batch_seq.keys():\n",
    "                batch_seq[key].append(ep[key][start:end])\n",
    "\n",
    "        tensor_batch = {}\n",
    "        for key, data_list in batch_seq.items():\n",
    "            stacked_data = np.stack(data_list)\n",
    "            \n",
    "            # --- FIX: Only flatten rewards and dones ---\n",
    "            if key in ['reward', 'done'] and stacked_data.ndim == 3 and stacked_data.shape[2] == 1:\n",
    "                stacked_data = stacked_data.reshape(batch_size, sequence_length)\n",
    "                \n",
    "            tensor_batch[key] = torch.tensor(stacked_data, dtype=torch.float32, device=device)\n",
    "\n",
    "        return tensor_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd800223",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mou_noise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OUNoise\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Critic\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from utils.ou_noise import OUNoise\n",
    "from mlp import Critic\n",
    "import itertools\n",
    "\n",
    "# --- Hyperparameters (from twc_td3_bptt_train.py) ---\n",
    "ENV = \"MountainCarContinuous-v0\"\n",
    "SEED = 42\n",
    "MAX_EPISODE        = 300\n",
    "MAX_TIME_STEPS     = 999\n",
    "WARMUP_STEPS       = 10_000 # Use this many steps to populate buffer\n",
    "BATCH_SIZE         = 128\n",
    "NUM_UPDATE_LOOPS   = 1 # Keep it simple for the notebook\n",
    "POLICY_DELAY       = 2 # TD3 Policy Delay\n",
    "\n",
    "GAMMA              = 0.99\n",
    "TAU                = 5e-3\n",
    "ACTOR_LR           = 3e-4 # Use a standard LR for this test\n",
    "CRITIC_LR          = 3e-4\n",
    "\n",
    "TWC_INTERNAL_STEPS = 1 # Keep it fast\n",
    "CRITIC_HID_LAYERS  = [256, 256] # Smaller critics for faster test\n",
    "DEVICE             = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SIGMA_START, SIGMA_END, SIGMA_DECAY_EPIS = 0.20, 0.05, 100\n",
    "\n",
    "# BPTT Hyperparameters\n",
    "SEQUENCE_LENGTH    = 40  # Total length of sequences to sample\n",
    "BURN_IN_LENGTH     = 10  # Number of steps to \"warm up\" the hidden state\n",
    "TRAIN_LENGTH       = SEQUENCE_LENGTH - BURN_IN_LENGTH\n",
    "\n",
    "# TD3 Noise Parameters\n",
    "TARGET_POLICY_NOISE = 0.2\n",
    "TARGET_NOISE_CLIP   = 0.5\n",
    "\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def detach_state_dict(state_dict: dict):\n",
    "    \"\"\"Detaches all tensors in a state dictionary from the computation graph.\"\"\"\n",
    "    return {\n",
    "        name: (state_pair[0].detach(), state_pair[1].detach())\n",
    "        for name, state_pair in state_dict.items()\n",
    "    }\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    \"\"\"Performs a soft update of target network parameters.\"\"\"\n",
    "    for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(tau * source_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_policy(eval_env, actor_net, n_episodes=10):\n",
    "    \"\"\"Runs a policy evaluation loop.\"\"\"\n",
    "    actor_net.eval() # Set model to eval mode\n",
    "    total_reward = 0.0\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = eval_env.reset()\n",
    "        actor_net.reset() # Reset the internal state for evaluation\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Use the stateful get_action_from_obs\n",
    "            action_tensor = actor_net.get_action_from_obs(\n",
    "                torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "            )\n",
    "            action = action_tensor.squeeze(0).cpu().numpy()\n",
    "            obs, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "    actor_net.train() # Set model back to train mode\n",
    "    return total_reward / n_episodes\n",
    "\n",
    "\n",
    "# ---\n",
    "# --- MAIN SCRIPT ---\n",
    "# ---\n",
    "\n",
    "print(\"--- Initializing ---\")\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# --- Init Env and Buffer ---\n",
    "env = gym.make(ENV)\n",
    "eval_env = gym.make(ENV)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "buffer = SequenceBuffer(capacity=100_000)\n",
    "ou_noise = OUNoise(action_dimension=act_dim, sigma=SIGMA_START)\n",
    "\n",
    "# --- Init Networks ---\n",
    "# Use the *real* encoders/decoders here\n",
    "actor = build_twc(\n",
    "    obs_encoder=twc.twc_io.mcc_obs_encoder_speed_weighted,\n",
    "    action_decoder=twc.twc_io.twc_out_2_mcc_action_tanh,\n",
    "    internal_steps=TWC_INTERNAL_STEPS,\n",
    "    log_stats=False\n",
    ").to(DEVICE)\n",
    "\n",
    "critic_1 = Critic(obs_dim, act_dim, size=CRITIC_HID_LAYERS).to(DEVICE)\n",
    "critic_2 = Critic(obs_dim, act_dim, size=CRITIC_HID_LAYERS).to(DEVICE)\n",
    "\n",
    "# Create target networks\n",
    "actor_target = build_twc(\n",
    "    obs_encoder=twc.twc_io.mcc_obs_encoder_speed_weighted,\n",
    "    action_decoder=twc.twc_io.twc_out_2_mcc_action_tanh,\n",
    "    internal_steps=TWC_INTERNAL_STEPS,\n",
    "    log_stats=False\n",
    ").to(DEVICE)\n",
    "critic_1_target = Critic(obs_dim, act_dim, size=CRITIC_HID_LAYERS).to(DEVICE)\n",
    "critic_2_target = Critic(obs_dim, act_dim, size=CRITIC_HID_LAYERS).to(DEVICE)\n",
    "\n",
    "# Initialize target networks\n",
    "actor_target.load_state_dict(actor.state_dict())\n",
    "critic_1_target.load_state_dict(critic_1.state_dict())\n",
    "critic_2_target.load_state_dict(critic_2.state_dict())\n",
    "\n",
    "# --- Init Optimizers ---\n",
    "actor_opt = optim.Adam(actor.parameters(), lr=ACTOR_LR)\n",
    "critic_opt = optim.Adam(itertools.chain(critic_1.parameters(), critic_2.parameters()), lr=CRITIC_LR)\n",
    "\n",
    "print(f\"Networks initialized. Running on {DEVICE}.\")\n",
    "\n",
    "\n",
    "# --- Populate Buffer ---\n",
    "print(f\"Populating buffer with {WARMUP_STEPS} random steps...\")\n",
    "obs, _ = env.reset()\n",
    "actor.reset() # Reset stateful actor\n",
    "for _ in range(WARMUP_STEPS):\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    buffer.store(obs, action, reward, next_obs, terminated, truncated)\n",
    "    obs = next_obs\n",
    "    if terminated or truncated:\n",
    "        obs, _ = env.reset()\n",
    "        actor.reset()\n",
    "print(f\"Buffer populated with {buffer.size} transitions.\")\n",
    "\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "print(\"--- Starting Training Loop ---\")\n",
    "total_steps = WARMUP_STEPS\n",
    "total_updates = 0\n",
    "\n",
    "for ep in range(MAX_EPISODE):\n",
    "    obs, _ = env.reset()\n",
    "    actor.reset()\n",
    "    ou_noise.update_sigma(ep)\n",
    "    ou_noise.reset()\n",
    "    \n",
    "    ep_reward = 0\n",
    "    ep_steps = 0\n",
    "    \n",
    "    for t in range(MAX_TIME_STEPS):\n",
    "        # 1. Get Action (using stateful evaluation method)\n",
    "        with torch.no_grad():\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "            action_tensor = actor.get_action_from_obs(obs_tensor)\n",
    "            action = action_tensor.squeeze(0).cpu().numpy()\n",
    "            action = (action + ou_noise.noise()).clip(-max_action, max_action)\n",
    "\n",
    "        # 2. Step Environment\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        ep_reward += reward\n",
    "        ep_steps += 1\n",
    "        total_steps += 1\n",
    "\n",
    "        # 3. Store in Buffer\n",
    "        buffer.store(obs, action, reward, next_obs, terminated, truncated)\n",
    "        obs = next_obs\n",
    "\n",
    "        # ---\n",
    "        # --- BPTT Update Step ---\n",
    "        # ---\n",
    "        \n",
    "        # We run the update logic (N times) *inside* the environment step loop\n",
    "        for _ in range(NUM_UPDATE_LOOPS):\n",
    "            try:\n",
    "                # 1. Sample a batch of sequences\n",
    "                batch_seq = buffer.sample(BATCH_SIZE, SEQUENCE_LENGTH, DEVICE)\n",
    "                obs_seq = batch_seq[\"obs\"]\n",
    "                act_seq = batch_seq[\"action\"]\n",
    "                rew_seq = batch_seq[\"reward\"]\n",
    "                obs2_seq = batch_seq[\"next_obs\"]\n",
    "                term_seq = batch_seq[\"done\"]\n",
    "                \n",
    "                # 2. Get initial hidden states (batched)\n",
    "                actor_state = actor.get_initial_state(BATCH_SIZE, DEVICE)\n",
    "                target_actor_state = actor_target.get_initial_state(BATCH_SIZE, DEVICE)\n",
    "\n",
    "                # 3. Burn-in Loop (No Gradients)\n",
    "                with torch.no_grad():\n",
    "                    for i in range(BURN_IN_LENGTH):\n",
    "                        _, actor_state = actor(obs_seq[:, i, :], actor_state)\n",
    "                        _, target_actor_state = actor_target(obs2_seq[:, i, :], target_actor_state)\n",
    "\n",
    "                # 4. Truncate (Detach)\n",
    "                actor_state = detach_state_dict(actor_state)\n",
    "                target_actor_state = detach_state_dict(target_actor_state)\n",
    "\n",
    "                # 5. Training Loop (With Gradients)\n",
    "                all_critic_losses = []\n",
    "                all_actor_losses = []\n",
    "\n",
    "                for i in range(BURN_IN_LENGTH, SEQUENCE_LENGTH):\n",
    "                    # Get data for this time step\n",
    "                    obs_t = obs_seq[:, i, :]\n",
    "                    act_t = act_seq[:, i, :]\n",
    "                    rew_t = rew_seq[:, i]\n",
    "                    obs2_t = obs2_seq[:, i, :]\n",
    "                    term_t = term_seq[:, i]\n",
    "                    mask_t = 1.0 - term_t\n",
    "\n",
    "                    # --- Compute Targets ---\n",
    "                    with torch.no_grad():\n",
    "                        noise = (torch.randn_like(act_t) * TARGET_POLICY_NOISE).clamp(-TARGET_NOISE_CLIP, TARGET_NOISE_CLIP)\n",
    "                        \n",
    "                        next_a_t, target_actor_state = actor_target(obs2_t, target_actor_state)\n",
    "                        next_a_t = (next_a_t + noise).clamp(-max_action, max_action)\n",
    "                        \n",
    "                        q1_t = critic_1_target(obs2_t, next_a_t)\n",
    "                        q2_t = critic_2_target(obs2_t, next_a_t)\n",
    "                        min_q = torch.minimum(q1_t, q2_t).squeeze(-1) # Squeeze Q-value\n",
    "                        \n",
    "                        q_target = rew_t + GAMMA * mask_t * min_q\n",
    "\n",
    "                    # --- Compute Critic Loss ---\n",
    "                    q1 = critic_1(obs_t, act_t).squeeze(-1) # Squeeze Q-value\n",
    "                    q2 = critic_2(obs_t, act_t).squeeze(-1) # Squeeze Q-value\n",
    "                    critic_loss_t = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n",
    "                    all_critic_losses.append(critic_loss_t)\n",
    "\n",
    "                    # --- Compute Actor Loss (Delayed) ---\n",
    "                    # We MUST unroll the actor to advance its state\n",
    "                    a_t, actor_state = actor(obs_t, actor_state)\n",
    "                    \n",
    "                    if total_updates % POLICY_DELAY == 0:\n",
    "                        actor_loss_t = -critic_1(obs_t, a_t).mean()\n",
    "                        all_actor_losses.append(actor_loss_t)\n",
    "\n",
    "                # 6. Backward Pass - Critic\n",
    "                critic_loss = torch.stack(all_critic_losses).mean()\n",
    "                \n",
    "                critic_opt.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(itertools.chain(critic_1.parameters(), critic_2.parameters()), 1.0)\n",
    "                critic_opt.step()\n",
    "\n",
    "                # 7. Backward Pass - Actor (Delayed)\n",
    "                total_updates += 1\n",
    "                if total_updates % POLICY_DELAY == 0:\n",
    "                    if all_actor_losses:\n",
    "                        actor_loss = torch.stack(all_actor_losses).mean()\n",
    "                        \n",
    "                        actor_opt.zero_grad()\n",
    "                        actor_loss.backward()\n",
    "                        nn.utils.clip_grad_norm_(actor.parameters(), 1.0)\n",
    "                        actor_opt.step()\n",
    "                        \n",
    "                        # Soft update all target networks\n",
    "                        soft_update(actor_target, actor, TAU)\n",
    "                        soft_update(critic_1_target, critic_1, TAU)\n",
    "                        soft_update(critic_2_target, critic_2, TAU)\n",
    "                \n",
    "            except ValueError as e:\n",
    "                # Buffer might not be ready\n",
    "                pass\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    # --- End of Episode ---\n",
    "    print(f\"Ep: {ep+1} | Total Steps: {total_steps} | Reward: {ep_reward:.2f}\")\n",
    "\n",
    "    if (ep+1) % 10 == 0:\n",
    "        eval_reward = evaluate_policy(eval_env, actor, n_episodes=10)\n",
    "        print(f\"----------------------------------------\")\n",
    "        print(f\"Evaluation after {ep+1} episodes: {eval_reward:.2f}\")\n",
    "        print(f\"----------------------------------------\")\n",
    "\n",
    "env.close()\n",
    "eval_env.close()\n",
    "print(\"--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819152f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyURI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
